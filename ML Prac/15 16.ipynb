{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 15 - Regularization on iris Dataset\n",
    "\n",
    "\n",
    "#### Type of regularization  used \n",
    "    L1 Regularization  (also called least absolute deviations)\n",
    "#### Regularization means :\n",
    "    This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting by minimizing its variance and changing bias a little.\n",
    "    \n",
    "##### Goal : \n",
    "    Run Logistic Regression With A L1 Penalty With Various Regularization Strengths(10,1,.1)\n",
    "    The usefulness of L1 is that it can push/penalize feature coefficients to 0, creating a method for feature selection. In the code below we run a logistic regression with a L1 penalty four times, each time decreasing the value of C. We should expect that as C decreases, more coefficients become 0.\n",
    "    \n",
    "##### Dataset used - same as 14th question - iris dataset -> will take Binary class only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "iris = load_iris()\n",
    "X = iris['data'][:100] \n",
    "Y = iris['target'][:100]\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shaurya singhal\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\shaurya singhal\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "s = StandardScaler()\n",
    "X = s.fit_transform(X)\n",
    "Y = s.fit_transform(Y.reshape(-1,1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for regularization\n",
    "def show():\n",
    "    penalty_strength = [10, 1, .1]\n",
    "\n",
    "    for i in penalty_strength:\n",
    "        model = LogisticRegression(solver = 'saga', penalty = 'l1', C = i, max_iter=1000)\n",
    "        model.fit(X_train, Y_train.reshape(-1))\n",
    "        print('strength : ' , i)\n",
    "        print('Coefficient of each feature:', model.coef_)\n",
    "        print('Training accuracy:', model.score(X_train, Y_train))\n",
    "        print('Test accuracy:', model.score(X_test, Y_test))\n",
    "        print()\n",
    "\n",
    "\n",
    "    print('Conclusion : ','as out penalty increases more #coefficent tends towards zero.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strength :  10\n",
      "Coefficient of each feature: [[ 0.         -1.07185362  3.53309363  3.300314  ]]\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n",
      "\n",
      "strength :  1\n",
      "Coefficient of each feature: [[ 0.         -0.54583485  2.39654506  1.89307965]]\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n",
      "\n",
      "strength :  0.1\n",
      "Coefficient of each feature: [[0.         0.         1.45119474 0.4847193 ]]\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n",
      "\n",
      "Conclusion :  as out penalty increases more #coefficent tends towards zero.\n"
     ]
    }
   ],
   "source": [
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queston 16 -> Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
