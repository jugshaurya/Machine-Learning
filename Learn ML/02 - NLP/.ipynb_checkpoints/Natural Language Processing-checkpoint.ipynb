{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "1) Natural language processing (NLP) concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "2) Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Aim : - to convert Text data to numerical data so as to give text documents to ML models.`\n",
    "\n",
    "\n",
    "where are those text documents ? : \n",
    "- just like sklearn have datasets module\n",
    "- nltk has download function , run it and download the data (all-corpora) -> collection of well-written text.\n",
    "\n",
    "![](images/nltk_download()1.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data in ` Brown Corpus`\n",
    " - The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown # brown has category wise data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "# to see the list of categories\n",
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.sents(categories='editorial') # load some category data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2997,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape # so it has 2997 eidtorial -sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:100] # lets take first 100 ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'General', 'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed', 'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from', 'the', 'day', 'it', 'convened', '.']\n"
     ]
    }
   ],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# NLP Pipeline\n",
    "```python\n",
    "- Doing anything complicated in machine learning usually means building a pipeline. The idea is to break up your problem into very small pieces and then use machine learning to solve each smaller piece separately. Then by chaining together several machine learning models that feed into each other, you can do very complicated things. And that’s exactly the strategy We’ll break down the process of understanding English into small chunks and see how each one works.\n",
    "```\n",
    "### Pipeline\n",
    "\n",
    "0. **`Data Collection`**(using nltk.corpus)\n",
    "\n",
    "\n",
    "1. **`Sentence tokenization`**(seperating lines whenever any '.' occur in sentence)(`nltk.tokenize.sent_tokenize()`)\n",
    "\n",
    "\n",
    "2. **`Word tokenization`**(seperating wods whenever any puntuation came up)(`nltk.tokenize.word_tokenize()`) \n",
    "\n",
    "\n",
    "3. **`Stopwords Removal`** (removing words which are not that important)(stopwods are in `nltk.corpus.stopwords.words()`)\n",
    "\n",
    "\n",
    "4. **`Puntuation Removal`**(removing puntuations) (using string.puntuation and list comprehension)\n",
    "\n",
    "\n",
    "5. **`Lemmatization or stemmming`**(Returning only the basic form ex - jumps,jumped,jumping all reduced to jump)(`nltk.stem.WordNetLemmatizer().lemmatize()`)\n",
    "\n",
    "**Using Bag of Words Model** : `(Constructing Vocabulary)`\n",
    "\n",
    "6. **`Building dictionary(Vocabulary)`** out of text : (`feature_extraction.text.CountVectorizer().fit_transform(text)`):(ngrams)\n",
    "\n",
    "\n",
    "7. **`Tf-idf Normalization`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1,2. Sentence and word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize # sentence tokenizer and word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hello shaurya, another miserable day ha!. Lets make it a fun day by learning NLP \\\n",
    "and maybe you got somthing useful this time. hello are you listening!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello shaurya, another miserable day ha!. Lets make it a fun day by learning NLP and maybe you got somthing useful this time. hello are you listening!\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello shaurya, another miserable day ha!.', 'Lets make it a fun day by learning NLP and maybe you got somthing useful this time.', 'hello are you listening!']\n",
      "['Hello', 'shaurya', ',', 'another', 'miserable', 'day', 'ha', '!', '.', 'Lets', 'make', 'it', 'a', 'fun', 'day', 'by', 'learning', 'NLP', 'and', 'maybe', 'you', 'got', 'somthing', 'useful', 'this', 'time', '.', 'hello', 'are', 'you', 'listening', '!']\n"
     ]
    }
   ],
   "source": [
    "print(sl)\n",
    "print(wl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)\n",
    "print(len(stop_words)) # these are the english stopwords which are not that important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flitering stopwords from word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_wl = [word for word in wl if word.lower() not in stop_words ] # removing all stopwards from li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'shaurya', ',', 'another', 'miserable', 'day', 'ha', '!', '.', 'Lets', 'make', 'fun', 'day', 'learning', 'NLP', 'maybe', 'got', 'somthing', 'useful', 'time', '.', 'hello', 'listening', '!']\n"
     ]
    }
   ],
   "source": [
    "print(removed_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'shaurya', ',', 'another', 'miserable', 'day', 'ha', '!', '.', 'Lets', 'make', 'it', 'a', 'fun', 'day', 'by', 'learning', 'NLP', 'and', 'maybe', 'you', 'got', 'somthing', 'useful', 'this', 'time', '.', 'hello', 'are', 'you', 'listening', '!']\n"
     ]
    }
   ],
   "source": [
    "print(wl)# that was our list with stopwords earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Puntuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punch_list = string.punctuation\n",
    "punch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'shaurya', ',', 'another', 'miserable', 'day', 'ha', '!', '.', 'Lets', 'make', 'fun', 'day', 'learning', 'NLP', 'maybe', 'got', 'somthing', 'useful', 'time', '.', 'hello', 'listening', '!']\n"
     ]
    }
   ],
   "source": [
    "print(removed_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_wl = [word for word in removed_wl if word not in punch_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'shaurya', 'another', 'miserable', 'day', 'ha', 'Lets', 'make', 'fun', 'day', 'learning', 'NLP', 'maybe', 'got', 'somthing', 'useful', 'time', 'hello', 'listening']\n"
     ]
    }
   ],
   "source": [
    "print(removed_wl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Lemmatization or Stemming\n",
    " - can use Snowball stemmer(multilingual)\n",
    " - can use Lancaster Stemmer\n",
    " - can use PorterStemmer\n",
    " - can use WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, LancasterStemmer , PorterStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SnowballStemmer('english')\n",
    "ls = LancasterStemmer()\n",
    "ps = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'shaurya', 'another', 'miserable', 'day', 'ha', 'Lets', 'make', 'fun', 'day', 'learning', 'NLP', 'maybe', 'got', 'somthing', 'useful', 'time', 'hello', 'listening']\n"
     ]
    }
   ],
   "source": [
    "print(removed_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('jumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cry'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('crying')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to do all above steps of NLP Pipelining over text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def NLP_preprocess(text):\n",
    "    text = text.lower()\n",
    "    # step 1 - wordTokenize\n",
    "    li = word_tokenize(text)\n",
    "    # step 2 - stopword removal\n",
    "    stopword_list = stopwords.words('english')\n",
    "    li = [word for word in li if word not in stopword_list]\n",
    "    # step 3 - puntuation Removal\n",
    "    punch_list = string.punctuation\n",
    "    li = [word for word in li if word not in punch_list]\n",
    "    # step 4 - Lemmatiziation\n",
    "    ss = SnowballStemmer('english')\n",
    "    li = list([ss.stem(word) for word in li ])\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello shaurya, another miserable day ha!. Lets make it a fun day by learning NLP and maybe you got somthing useful this time. hello are you listening!'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = NLP_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'shaurya', 'anoth', 'miser', 'day', 'ha', 'let', 'make', 'fun', 'day', 'learn', 'nlp', 'mayb', 'got', 'somth', 'use', 'time', 'hello', 'listen']\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello shaurya anoth miser day ha let make fun day learn nlp mayb got somth use time hello listen'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = ' '.join(ans)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words Model\n",
    "```\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Building Vocabulary(dictionary of word along with its frequencies in a text)\n",
    "\n",
    "# (`using Bags of Words model` implemented in sklearn)\n",
    "- from sklearn.feature_extraction.text import CountVectorizer\n",
    "- CountVectorizer().fit_transform(text)\n",
    "- CountVectorizer().get_feature_names()\n",
    "- CountVectorizer().vocabulary_\n",
    "- CountVectorizer().inverse_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer() \n",
    "# if words of text are arranged in a dictionary(real one in life!) then position of word is counted by this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello shaurya anoth miser day ha let make fun day learn nlp mayb got somth use time hello listen\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 1)\t2\n",
      "  (0, 11)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 5)\t2\n"
     ]
    }
   ],
   "source": [
    "print(cv.fit_transform([ans]))  # (0,1) means in 0th sentence of ans, 1st dictionary element is present 2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 5, 'shaurya': 13, 'anoth': 0, 'miser': 11, 'day': 1, 'ha': 4, 'let': 7, 'make': 9, 'fun': 2, 'learn': 6, 'nlp': 12, 'mayb': 10, 'got': 3, 'somth': 14, 'use': 16, 'time': 15, 'listen': 8}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_) # printing word with dictionary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anoth', 'day', 'fun', 'got', 'ha', 'hello', 'learn', 'let', 'listen', 'make', 'mayb', 'miser', 'nlp', 'shaurya', 'somth', 'time', 'use']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names()) # printing unique words in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello shaurya anoth miser day ha let make fun day learn nlp mayb got somth use time hello listen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating a vector for our ans \n",
    "print(ans)\n",
    "arr = cv.transform([ans]).toarray()\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 5,\n",
       " 'shaurya': 13,\n",
       " 'anoth': 0,\n",
       " 'miser': 11,\n",
       " 'day': 1,\n",
       " 'ha': 4,\n",
       " 'let': 7,\n",
       " 'make': 9,\n",
       " 'fun': 2,\n",
       " 'learn': 6,\n",
       " 'nlp': 12,\n",
       " 'mayb': 10,\n",
       " 'got': 3,\n",
       " 'somth': 14,\n",
       " 'use': 16,\n",
       " 'time': 15,\n",
       " 'listen': 8}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['anoth', 'day', 'fun', 'got', 'ha', 'hello', 'learn', 'let',\n",
       "        'listen', 'make', 'mayb', 'miser', 'nlp', 'shaurya', 'somth',\n",
       "        'time', 'use'], dtype='<U7')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(arr) # it maps every 1 or more to dictionary and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams : Bag Of Words Model\n",
    "- making dictionary in which each word of dictionary is made up of single word, which we are doing till now\n",
    "\n",
    "# N-grams : Bag Of Words Model(Bigrams ,trigrams....)\n",
    "- making dictionary in which each word of dictionary is made up of n-word `use CountVectorize(ngrma_range=(1,2))` to get words of length 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Taunton England.',\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "        'The nobel laurate won the hearts of the people',\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unigram + Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=NLP_preprocess, ngram_range=(1,2)) # tokenizer to do above 5 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Taunton England.',\n",
       " 'We will win next Lok Sabha Elections, says confident Indian PM',\n",
       " 'The nobel laurate won the hearts of the people',\n",
       " 'The movie Raazi is an exciting Indian Spy thriller based upon a real story']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x65 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 69 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 20, 'cricket': 6, 'team': 52, 'win': 60, 'world': 63, 'cup': 8, 'say': 44, 'capt': 2, 'virat': 58, 'koh': 24, 'held': 18, 'taunton': 50, 'england': 13, 'indian cricket': 21, 'cricket team': 7, 'team win': 53, 'win world': 62, 'world cup': 64, 'cup say': 10, 'say capt': 45, 'capt virat': 3, 'virat koh': 59, 'koh world': 25, 'cup held': 9, 'held taunton': 19, 'taunton england': 51, 'next': 32, 'lok': 28, 'sabha': 42, 'elect': 11, 'confid': 4, 'pm': 37, 'win next': 61, 'next lok': 33, 'lok sabha': 29, 'sabha elect': 43, 'elect say': 12, 'say confid': 46, 'confid indian': 5, 'indian pm': 22, 'nobel': 34, 'laurat': 26, 'heart': 16, 'peopl': 36, 'nobel laurat': 35, 'laurat heart': 27, 'heart peopl': 17, 'movi': 30, 'raazi': 38, 'excit': 14, 'spi': 47, 'thriller': 54, 'base': 0, 'upon': 56, 'real': 40, 'stori': 49, 'movi raazi': 31, 'raazi excit': 39, 'excit indian': 15, 'indian spi': 23, 'spi thriller': 48, 'thriller base': 55, 'base upon': 1, 'upon real': 57, 'real stori': 41}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unigram + Bigram + Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=NLP_preprocess, ngram_range=(1,3)) # 1 to 3 gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x95 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 99 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 29, 'cricket': 9, 'team': 74, 'win': 86, 'world': 91, 'cup': 12, 'say': 63, 'capt': 3, 'virat': 83, 'koh': 35, 'held': 26, 'taunton': 72, 'england': 20, 'indian cricket': 30, 'cricket team': 10, 'team win': 75, 'win world': 89, 'world cup': 92, 'cup say': 15, 'say capt': 64, 'capt virat': 4, 'virat koh': 84, 'koh world': 36, 'cup held': 13, 'held taunton': 27, 'taunton england': 73, 'indian cricket team': 31, 'cricket team win': 11, 'team win world': 76, 'win world cup': 90, 'world cup say': 94, 'cup say capt': 16, 'say capt virat': 65, 'capt virat koh': 5, 'virat koh world': 85, 'koh world cup': 37, 'world cup held': 93, 'cup held taunton': 14, 'held taunton england': 28, 'next': 47, 'lok': 41, 'sabha': 60, 'elect': 17, 'confid': 6, 'pm': 54, 'win next': 87, 'next lok': 48, 'lok sabha': 42, 'sabha elect': 61, 'elect say': 18, 'say confid': 66, 'confid indian': 7, 'indian pm': 32, 'win next lok': 88, 'next lok sabha': 49, 'lok sabha elect': 43, 'sabha elect say': 62, 'elect say confid': 19, 'say confid indian': 67, 'confid indian pm': 8, 'nobel': 50, 'laurat': 38, 'heart': 24, 'peopl': 53, 'nobel laurat': 51, 'laurat heart': 39, 'heart peopl': 25, 'nobel laurat heart': 52, 'laurat heart peopl': 40, 'movi': 44, 'raazi': 55, 'excit': 21, 'spi': 68, 'thriller': 77, 'base': 0, 'upon': 80, 'real': 58, 'stori': 71, 'movi raazi': 45, 'raazi excit': 56, 'excit indian': 22, 'indian spi': 33, 'spi thriller': 69, 'thriller base': 78, 'base upon': 1, 'upon real': 81, 'real stori': 59, 'movi raazi excit': 46, 'raazi excit indian': 57, 'excit indian spi': 23, 'indian spi thriller': 34, 'spi thriller base': 70, 'thriller base upon': 79, 'base upon real': 2, 'upon real stori': 82}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Bigram  +Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=NLP_preprocess, ngram_range=(2,3)) # tokenizer to do above 5 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x63 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 63 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian cricket': 19, 'cricket team': 6, 'team win': 48, 'win world': 58, 'world cup': 60, 'cup say': 10, 'say capt': 41, 'capt virat': 2, 'virat koh': 54, 'koh world': 24, 'cup held': 8, 'held taunton': 17, 'taunton england': 47, 'indian cricket team': 20, 'cricket team win': 7, 'team win world': 49, 'win world cup': 59, 'world cup say': 62, 'cup say capt': 11, 'say capt virat': 42, 'capt virat koh': 3, 'virat koh world': 55, 'koh world cup': 25, 'world cup held': 61, 'cup held taunton': 9, 'held taunton england': 18, 'win next': 56, 'next lok': 32, 'lok sabha': 28, 'sabha elect': 39, 'elect say': 12, 'say confid': 43, 'confid indian': 4, 'indian pm': 21, 'win next lok': 57, 'next lok sabha': 33, 'lok sabha elect': 29, 'sabha elect say': 40, 'elect say confid': 13, 'say confid indian': 44, 'confid indian pm': 5, 'nobel laurat': 34, 'laurat heart': 26, 'heart peopl': 16, 'nobel laurat heart': 35, 'laurat heart peopl': 27, 'movi raazi': 30, 'raazi excit': 36, 'excit indian': 14, 'indian spi': 22, 'spi thriller': 45, 'thriller base': 50, 'base upon': 0, 'upon real': 52, 'real stori': 38, 'movi raazi excit': 31, 'raazi excit indian': 37, 'excit indian spi': 15, 'indian spi thriller': 23, 'spi thriller base': 46, 'thriller base upon': 51, 'base upon real': 1, 'upon real stori': 53}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can construct a vector of (length of dictionary) in which 1 represent at ith index the presence of ith word of dictionary in our sentence and 0 the absence (this is Vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian cricket': 19, 'cricket team': 6, 'team win': 48, 'win world': 58, 'world cup': 60, 'cup say': 10, 'say capt': 41, 'capt virat': 2, 'virat koh': 54, 'koh world': 24, 'cup held': 8, 'held taunton': 17, 'taunton england': 47, 'indian cricket team': 20, 'cricket team win': 7, 'team win world': 49, 'win world cup': 59, 'world cup say': 62, 'cup say capt': 11, 'say capt virat': 42, 'capt virat koh': 3, 'virat koh world': 55, 'koh world cup': 25, 'world cup held': 61, 'cup held taunton': 9, 'held taunton england': 18, 'win next': 56, 'next lok': 32, 'lok sabha': 28, 'sabha elect': 39, 'elect say': 12, 'say confid': 43, 'confid indian': 4, 'indian pm': 21, 'win next lok': 57, 'next lok sabha': 33, 'lok sabha elect': 29, 'sabha elect say': 40, 'elect say confid': 13, 'say confid indian': 44, 'confid indian pm': 5, 'nobel laurat': 34, 'laurat heart': 26, 'heart peopl': 16, 'nobel laurat heart': 35, 'laurat heart peopl': 27, 'movi raazi': 30, 'raazi excit': 36, 'excit indian': 14, 'indian spi': 22, 'spi thriller': 45, 'thriller base': 50, 'base upon': 0, 'upon real': 52, 'real stori': 38, 'movi raazi excit': 31, 'raazi excit indian': 37, 'excit indian spi': 15, 'indian spi thriller': 23, 'spi thriller base': 46, 'thriller base upon': 51, 'base upon real': 1, 'upon real stori': 53}\n",
      "\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)\n",
    "length = len(cv.vocabulary_)\n",
    "print()\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "vector = np.random.randint(0,2,length)\n",
    "vector # Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = cv.inverse_transform(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.array(arr).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'base upon real.....cricket team win.....cup held.....cup say.....cup say capt.....elect say confid.....excit indian spi.....held taunton.....held taunton england.....indian spi.....indian spi thriller.....koh world.....laurat heart.....movi raazi excit.....next lok.....next lok sabha.....raazi excit.....sabha elect.....say capt.....say confid.....say confid indian.....spi thriller.....thriller base.....thriller base upon.....virat koh.....win next.....win world cup.....world cup held'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.....'.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf Normalization (Term Frequency - Inverse Document Frequency)(classifier)\n",
    "\n",
    "- `Term frequency` means occurence of a term or word in a complete Document.(the number of times a term occurs in a document is called its term frequency).(The weight of a term that occurs in a document is simply proportional to the term frequency).\n",
    "\n",
    "\n",
    "- `Inverse Document Frequency Factor ` means diminishes the weight of terms that occur very frequently in the document sentence and increases the weight of terms that occur rarely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
