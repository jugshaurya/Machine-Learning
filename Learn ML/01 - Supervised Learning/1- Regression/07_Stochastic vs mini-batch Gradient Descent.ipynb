{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[nptel explains it better](https://www.youtube.com/watch?v=Vc-jG_LdOLw&list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT&index=39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent  we learned till now is Batch(complete sample) Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "earlier in Gradient descent algorithm we were making one update(taking one step toward minima) to our thetas by going through all the samples of our data, suppose if we have millions of sample points then to make a single update we go over the complete sample, it is time consuming.\n",
    "but why were we doing that !! beacuse that was the right thing to do !! as we derived that using error function, so yes that should have happened.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y,theta):\n",
    "    grad = np.zeros(2)\n",
    "    hx = theta[0] + theta[1]*X\n",
    "    grad[0] = np.sum(hx-Y)\n",
    "    grad[1] = np.sum((hx-Y)*X)\n",
    "    return grad\n",
    "\n",
    "# code similar to we written in 3rd notebook for univarate gradient descent\n",
    "def batch_gradientDescent(X,Y,learning_rate=0.0001):\n",
    "    theta = np.zeros(2)\n",
    "    itr = 0\n",
    "    max_itr = 1000\n",
    "    \n",
    "    while itr<max_itr:        \n",
    "        # calulate the gradent over complete sample\n",
    "        grad = gradient(X,Y,theta)\n",
    "        # updating after it\n",
    "        theta[0] = theta[0] - learning_rate * grad[0] # grad 0 is d(cost) / d(theta 0)\n",
    "        theta[1] = theta[1] - learning_rate * grad[1] # grad 1 is d(cost) / d(theta 1)        \n",
    "        itr += 1\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic(Approximation) Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "now instead of going through the complete sample and then making a update, we go for an approximation,and  update our theta for each and every sample points .\n",
    "what will happen??\n",
    "    - now there is no guarantee that each update will decrease our loss , or we move toward the minima !! why? !! beacuse now every single sample point is trying to push the parameters(theta) in the direction more favourable to it, without being thinking how will it affect other points. \n",
    "    \n",
    "    - so for now  in one single step of moving toward minima , we are doing oscillation before taking a single step.\n",
    "This is called stochastic gradient Descent.\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y,theta):\n",
    "    grad = np.zeros(2)\n",
    "    hx = theta[0] + theta[1]*X\n",
    "    grad[0] = np.sum(hx-Y)\n",
    "    grad[1] = np.sum((hx-Y)*X)\n",
    "    return grad\n",
    "\n",
    "def stochastic_gradientDescent(X,Y,learning_rate=0.0001):\n",
    "    theta = np.zeros(2)\n",
    "    epoch = 0 # epoch means iteration\n",
    "    max_epoch = 1000\n",
    "    \n",
    "    while epoch<max_epoch:\n",
    "        for each_sample in range(X.shape[0]):\n",
    "            grad = gradient(X[each_sample],Y[each_sample],theta)\n",
    "            # updating for each example after it\n",
    "            theta[0] = theta[0] - learning_rate * grad[0] # grad 0 is d(cost) / d(theta 0)\n",
    "            theta[1] = theta[1] - learning_rate * grad[1] # grad 1 is d(cost) / d(theta 1)        \n",
    "        epoch += 1\n",
    "        \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "we can do better than stochastic by making an update to parameters in some batches(dividing our sample into some some batches) instead of doing that at every single sample point, this way we would have less oscillations than stochastic ,but we will have the trajectory of this mini-batch contained inside the stochastic (because of less oscillations).\n",
    "\n",
    "\n",
    "so we can say that if batch size is one , mini-batch is similar to stochastic Gradient Descent.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,Y,theta):\n",
    "    grad = np.zeros(2)\n",
    "    hx = theta[0] + theta[1]*X\n",
    "    grad[0] = np.sum(hx-Y)\n",
    "    grad[1] = np.sum((hx-Y)*X)\n",
    "    return grad\n",
    "\n",
    "def mini_batch_gradientDescent(X,Y,learning_rate=0.0001,batch_size = 4):\n",
    "    theta = np.zeros(2)\n",
    "    epoch = 0\n",
    "    max_epoch = 1000\n",
    "    \n",
    "    while epoch<max_epoch:\n",
    "        grad = np.zeros(2)\n",
    "        for each_sample in range(X.shape[0]):\n",
    "            # calculating graient over the current batch\n",
    "            grad += gradient(X[each_sample],Y[each_sample],theta)\n",
    "            \n",
    "            # updating for each batch only \n",
    "            if (each_sample % batch_size) == 0 :\n",
    "                theta[0] = theta[0] - learning_rate * grad[0] # grad 0 is d(cost) / d(theta 0)\n",
    "                theta[1] = theta[1] - learning_rate * grad[1] # grad 1 is d(cost) / d(theta 1)  \n",
    "                grad = (0,0) # resetting gradient\n",
    "            \n",
    "        epoch += 1\n",
    "        \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/stochastic_mini_batch.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
